\section{Approach}

\subsection{Model Architecture}

Our recipe generation system is built upon GPT-2, a transformer based causal language model developed by OpenAI. Specifically, we utilize the pre trained \texttt{mbien/recipenlg} model, which is a fine tuned variant of GPT-2 medium (176 million parameters) tailored for generating semi structured culinary text. This model originates from the RecipeNLG project, which aims to address the structured generation of cooking recipes using language models.

GPT-2 employs a transformer decoder architecture with self attention mechanisms that allow it to model long range dependencies in text. As a causal language model, it generates text autoregressivelyâ€”predicting the next token based on all previously generated tokens. This property is especially beneficial for generating recipes, which require coherent sequencing from ingredients to step by step instructions.

The architecture is well uited to structured generation tasks, as it can learn and replicate the patterns typical of recipe text, such as consistent formatting of ingredients, procedural clarity in instructions, and contextual compatibility among ingredients. The model's training on a large corpus of recipes enables it to internalize domain specific language and conventions, making it an effective base for further fine-tuning on health-constrained recipe data.
